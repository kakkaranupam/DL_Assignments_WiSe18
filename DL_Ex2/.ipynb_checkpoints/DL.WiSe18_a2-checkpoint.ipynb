{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise (Chapter 6 & 8)\n",
    "\n",
    "In this exercise we consider Chapters 6 & 8 of the book \"Deep Learning\". The exercise focuses on implementing a small feedforward neural network and training it on the MNIST dataset.\n",
    "\n",
    "We provide code for structure and utility, you have to **fill in the TODO-gaps**.\n",
    "It might initially look like a lot of unnecessary code, but it keeps the network extensible. In the following exercises you can reuse what you've done here. Most common neural network libraries (*pytorch*, *tensorflow/keras*, …) are similarly structured, hence they will be easy to use once you've finished this notebook. As we will be using *pytorch* towards the end of the lecture, our API resembles the API of the pytorch framework.\n",
    "\n",
    "\n",
    "We will implement two different cost functions and play a bit with the value of the different hyperparameters to see how performances change as a function of those. In particular, we will focus on\n",
    "\n",
    "* Sigmoid Neurons\n",
    "* Stochastic Gradient Descent\n",
    "* Quadratic Cost Function\n",
    "* Cross Entropy Cost Function\n",
    "* Learning Rate\n",
    "* Generalization\n",
    "\n",
    "Note that we'll implement all of these operations to operate on mini-batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports used in the code below\n",
    "\n",
    "from typing import Iterable, List, Optional, Tuple  # type annotations\n",
    "import pickle  # data loading\n",
    "import gzip  # data loading\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import scipy.optimize  # gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the two main classes to structure your network. \n",
    "\n",
    "**Parameter** is used to represent trainable variables in the network, e.g., a layer's weights $w$. The weights themself are a *numpy array* as the parameter's `data` attribute. The associated parameter gradient (e.g. $\\frac{\\partial L}{\\partial w}$) can be stored in the `grad` attribute.\n",
    "\n",
    "\n",
    "\n",
    "**Module** is the base class for all parts of the network (activations, layers, …) and even the network itself. They all have to implement the `forward` and `backward` methods. For backpropagation activation will flow *forward* and gradient will flow *backward* through the *network graph and it's modules*. \n",
    "\n",
    "Additional module provides utility to check the correctness of implementation by approximating *backward* with [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference#Relation_with_derivatives)  of *forward*.\n",
    "\n",
    "*Note:* All modules operate on batches of samples. E.g. the input shape of `Linear.forward` is `(batch_size, feature_shape, 1)` (we will use the last dimension in future exercises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores the value of the parameter (self.data) but also tensors/\n",
    "    properties associated with it, such as the gradient (self.grad) of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, name=None, grad: Optional[np.ndarray] = None):\n",
    "        self.data = data  # type: np.ndarray\n",
    "        self.grad = grad  # type: Optional[np.ndarray]\n",
    "        self.name = name  # type: Optional[str]\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "        \n",
    "        \n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        print(\"len(self.parameters()) / \", str(len(self.parameters())))\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                ogp = np.sum(self.forward(*input_args))\n",
    "                return ogp\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "#                 for p in self.parameters():\n",
    "#                     print(\"param / \", p)\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "#                 print(param.name, \" param.data.shape /\", param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "#                 print(\"out.shape /\", str(out.shape))\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                ggp = np.ravel(param.grad)\n",
    "#                 print(\"ggp.shape /\", str(ggp.shape))\n",
    "                return ggp\n",
    "            \n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            print(\"param_init.shape / \" + str(param_init.shape))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinearities\n",
    "\n",
    "## Sigmoid \n",
    "\n",
    "Implement your first network module: The sigmoid activation function.\n",
    "\n",
    "Verify your sigmoid function by plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "        # END TODO###################\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        assert len(z.shape) == 3, (\"z.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(z.shape))\n",
    "        h = self._sigmoid(z)\n",
    "        # here it's useful to store the activation \n",
    "        #  instead of the input\n",
    "        self.input_cache = h\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        h = self.input_cache\n",
    "#         print(\"SIG backward /\", grad.shape)\n",
    "        # START TODO ################\n",
    "        sigd = h * (1.0 - h)\n",
    "        return grad * sigd\n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-4, +4, 100)\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "y = np.ravel(sigmoid(x.reshape(-1, 1, 1)))\n",
    "# equal to call of sigmoid.forward(x)\n",
    "    \n",
    "plt.plot(x,y)\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu\n",
    "\n",
    "Implement Relu and plot for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def _relu(self, z: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        # print(\"#METOO\")\n",
    "        return np.maximum(0.0, z)\n",
    "        # END TODO###################\n",
    "        \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        # START TODO ################\n",
    "        # print(\"CALLED\")\n",
    "        return self._relu(z)\n",
    "        # END TODO###################\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        z = self.input_cache\n",
    "#         print(\"ReLU backward /\", grad.shape)\n",
    "        # START TODO ################\n",
    "        grad_r = np.ones_like(z)\n",
    "        grad_r[z < 0.0] = 0.0\n",
    "        return grad_r * grad\n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Relu\n",
    "\n",
    "# START TODO ################\n",
    "x = np.linspace(-4, +4, 100)\n",
    "relu = Relu()\n",
    "\n",
    "y = np.ravel(relu(x.reshape(-1, 1, 1)))\n",
    "# equal to call of sigmoid.forward(x)\n",
    "    \n",
    "plt.plot(x,y)\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Implement the numerical stable softmax. We will not need the backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def _softmax(self, z):\n",
    "        # don't reduce (sum) over batch axis\n",
    "        reduction_axes = tuple(range(1, len(z.shape))) \n",
    "        \n",
    "        # START TODO ################\n",
    "        z_max = np.max(z, axis=reduction_axes, keepdims=True)\n",
    "        # Shift input for numerical stability.\n",
    "        z_safe = z - z_max\n",
    "        e_z = np.exp(z_safe)\n",
    "        h = e_z / np.sum(e_z, axis=reduction_axes, keepdims=True)\n",
    "        # END TODO###################\n",
    "        return h\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        h = self._softmax(z)\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad) -> np.ndarray:\n",
    "        error_msg = (\"Softmax doesn't need to implement a gradient here, as it's\"\n",
    "                     \"only needed in CrossEntropyLoss, where we can simplify\"\n",
    "                     \"the gradient for the combined expression.\")\n",
    "        raise NotImplementedError(error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your softmax\n",
    "softmax = Softmax()\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]).reshape(1, -1, 1)\n",
    "\n",
    "# Testcase from https://en.wikipedia.org/wiki/Softmax_function#Example\n",
    "np.testing.assert_allclose(\n",
    "    np.ravel(softmax(x)), \n",
    "    [0.02364054, 0.06426166, 0.1746813, 0.474833, 0.02364054, 0.06426166, 0.1746813],\n",
    "    rtol=1e-5, err_msg=\"Softmax is not correct implemented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "Implement a linear (in other frameworks also called dense or fully connected) network layer. \n",
    "Here you also have to use the Parameter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        w_data = 0.01 * np.random.randn(out_features, in_features)\n",
    "        self.W = Parameter(w_data, \"W\")\n",
    "        \n",
    "        b_data = 0.01 * np.ones((out_features, 1))\n",
    "        self.b = Parameter(b_data, \"b\")\n",
    "        \n",
    "        print(\"init / self.W.shape\", self.W.data.shape)\n",
    "        print(\"init / self.b.shape\", self.b.data.shape)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "        self.input_cache = x\n",
    "        # START TODO ################\n",
    "        # Remember: Access weight data through self.W.data\n",
    "        # z = np.dot(self.input_cache, self.W.data) + self.b.data\n",
    "        z = (self.W.data @ self.input_cache) + self.b.data\n",
    "\n",
    "#         print(\"LIN forward:\")\n",
    "#         print(\"self.W.data.shape : \" + str(self.W.data.shape))\n",
    "#         print(\"self.input_cache.shape : \" + str(self.input_cache.shape))\n",
    "#         print(\"self.b.data.shape : \" + str(self.b.data.shape))\n",
    "#         print(\"z = (self.W.data @ self.input_cache) + self.b.data\")\n",
    "#         print(\"z.shape : \" + str(z.shape))\n",
    "\n",
    "        return z\n",
    "        # END TODO ##################\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "        # remember that input has a batch dimension when transposing, i.e.,\n",
    "        # we need to use np.transpose instead of x.T\n",
    "        x_transpose = np.transpose(x, [0, 2, 1])\n",
    "        # START TODO ################ \n",
    "        # self.W.grad += ...\n",
    "        # ..\n",
    "#         print(\"LIN backward B4\")\n",
    "#         print(\"dz.shape : \" + str(dz.shape))\n",
    "#         print(\"dz.T.shape : \" + str(dz.T.shape))\n",
    "#         print(\"x.shape : \" + str(x.shape))\n",
    "#         print(\"x_transpose.shape : \" + str(x_transpose.shape))\n",
    "#         print(\"n.shape : \" + str(n))\n",
    "#         print(\"self.W.grad.shape : \" + str(self.W.grad.shape))\n",
    "#         print(\"self.b.grad.shape : \" + str(self.b.grad.shape))\n",
    "#         print(\"self.W.data.shape :\", self.W.data.shape)\n",
    "#         print(\"self.W.data.T.shape :\", self.W.data.T.shape)\n",
    "        dz = self.W.data.T @ grad\n",
    "        self.W.grad += np.sum((grad @ x_transpose), axis=0)\n",
    "        self.b.grad += np.sum(grad, axis=0)\n",
    "#         print(\"LIN backward AFTER\")\n",
    "#         print(\"dz.shape : \" + str(dz.shape))\n",
    "#         print(\"self.W.grad.shape : \" + str(self.W.grad.shape))\n",
    "#         print(\"self.b.grad.shape : \" + str(self.b.grad.shape))\n",
    "        return dz\n",
    "        # END TODO ##################\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # START TODO ################ \n",
    "        # Return all parameters of Linear\n",
    "        # return [self.W, self.b]\n",
    "#         print(\"params / \" + str(len(np.ravel([self.W, self.b]))))\n",
    "        return [self.W, self.b]\n",
    "        # END TODO ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cross-Entropy cost functions\n",
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        return -np.sum(np.log(a[y == 1])) / len(a)\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg.\n",
    "        a, y = self.input_cache\n",
    "        \n",
    "        # START TODO ################\n",
    "        # note to self : also check for division by n (shape[0])\n",
    "        cel_grad = (a - y) / len(y)\n",
    "        cel_grad = cel_grad[:,:,np.newaxis]\n",
    "        # Recreate the batch dimension\n",
    "        # END TODO ##################\n",
    "        \n",
    "        assert len(cel_grad.shape) == 3, (\"CrossEntropyLoss.backward should return (batch_size, grad_size, 1)\"\n",
    "                                      \" but is {}.\".format(cel_grad.shape))\n",
    "        return cel_grad\n",
    "\n",
    "\n",
    "class MSELoss(Module):\n",
    "    \"\"\"Compute the mean squared error loss.\"\"\"\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = a, y\n",
    "        return np.sum(0.5 * np.linalg.norm(a - y, axis=-1)**2) / len(a)\n",
    "\n",
    "    def backward(self, _=None):\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg\n",
    "        a, y = self.input_cache\n",
    "        \n",
    "        # START TODO ################ \n",
    "        # raise NotImplementedError\n",
    "        # note to self : also check for division by n (shape[0])\n",
    "        msel_grad = (a - y) / len(y)\n",
    "        msel_grad = msel_grad[:,:,np.newaxis]\n",
    "        # Recreate the batch dimension\n",
    "        # END TODO ##################\n",
    "        \n",
    "        assert len(msel_grad.shape) == 3, (\"MSELossCrossEntropyLoss.backward should return (batch_size, grad_size, 1)\"\n",
    "                                      \" but is {}.\".format(msel_grad.shape))\n",
    "        return msel_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        for mod in self.modules:\n",
    "            x = mod.forward(x)\n",
    "        return x\n",
    "        raise NotImplementedError\n",
    "        # Remember: module(x) is equivalent to module.forward(x)\n",
    "        # END TODO ##################\n",
    "        # return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "#         print(\"SEQ backward /\", grad.shape)\n",
    "        n = len(self.modules)\n",
    "        for i in range(n-1, -1, -1):\n",
    "            grad = self.modules[i].backward(grad)\n",
    "        return grad\n",
    "        # END TODO ##################\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding\n",
    "To handle categorical data, we need to implement the one_hot_encoding utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2] --> [[0, 1, 0], [0, 0, 1]]\n",
    "    \"\"\"\n",
    "    # START TODO ################\n",
    "    classes = np.unique(y)\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    for c in classes:\n",
    "        encoded[y == c, c] = 1\n",
    "    return encoded\n",
    "    raise NotImplementedError\n",
    "    # END TODO ##################\n",
    "\n",
    "y = np.array([1, 2, 0])\n",
    "np.testing.assert_equal(one_hot_encoding(y, 3), [[0, 1, 0], [0, 0, 1], [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check\n",
    "\n",
    "Gradient checking is a useful utility to check, whether forward and backward pass are matching. \n",
    "Backward passes are approximated by finite differences of the forward pass.\n",
    "We have implemented the gradient checking in the Module class for you. As all classes we defined up to here inherit from Module, we can run `check_gradients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i/p vec shape (2, 10, 1) (20,)\n",
      "len(self.parameters()) /  0\n",
      "len(self.parameters()) /  0\n",
      "init / self.W.shape (20, 10)\n",
      "init / self.b.shape (20, 1)\n",
      "len(self.parameters()) /  2\n",
      "param_init.shape / (200,)\n",
      "param_init.shape / (20,)\n",
      "init / self.W.shape (30, 10)\n",
      "init / self.b.shape (30, 1)\n",
      "init / self.W.shape (20, 30)\n",
      "init / self.b.shape (20, 1)\n",
      "len(self.parameters()) /  4\n",
      "param_init.shape / (300,)\n",
      "param_init.shape / (30,)\n",
      "param_init.shape / (600,)\n",
      "param_init.shape / (20,)\n",
      "len(self.parameters()) /  0\n",
      "len(self.parameters()) /  0\n"
     ]
    }
   ],
   "source": [
    "input_vector = np.random.uniform(-1., 1., size=(2, 10, 1))\n",
    "input_args = (input_vector,)\n",
    "print(\"i/p vec shape\", input_vector.shape, np.ravel(input_vector).shape)\n",
    "\n",
    "# layers + activations\n",
    "Relu().check_gradients(input_args)\n",
    "Sigmoid().check_gradients(input_args)\n",
    "Linear(10, 20).check_gradients(input_args)\n",
    "\n",
    "# MLP\n",
    "# START TODO ################\n",
    "Sequential(Linear(10,30),Sigmoid(),Linear(30,20),Sigmoid()).check_gradients(input_args)\n",
    "# gradient check a Sequential network with \n",
    "# layers: linear, sigmoid, linear, sigmoid\n",
    "# END TODO ##################\n",
    "\n",
    "# losses\n",
    "input_args_losses = (one_hot_encoding(np.array([1, 2]), 3),  # a\n",
    "                     one_hot_encoding(np.array([1, 1]), 3))  # y (ground truth)\n",
    "MSELoss().check_gradients(input_args_losses)\n",
    "CrossEntropyLoss().check_gradients(input_args_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Below you see the base class for all optimizers. An Optimizer needs to implement the `step()` function, which updates the parameters passed to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"The base class for optimizers.\n",
    "\n",
    "    All optimizers must implement a step() method that updates the parameters.\n",
    "    The general optimization loop then looks like this:\n",
    "\n",
    "    for inputs, targets in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    `zero_grad` initializes the gradients of the parameters to zero. This\n",
    "    allows to accumulate gradients (instead of replacing it) during\n",
    "    backpropagation, which is e.g. useful for skip connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: The parameters to be optimized.\n",
    "        \"\"\"\n",
    "        self._params = params\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update the parameters.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "        for param in self._params:\n",
    "            assert isinstance(param, Parameter)\n",
    "            param.grad = np.zeros_like(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "Implement stochastic gradient descent (Chapter 5, Chapter 8). \n",
    "The momentum parameter improves training speed (Chapter 8.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) optimizer with optional Momentum.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float,\n",
    "                 momentum: Optional[float] = None):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        if momentum:\n",
    "            for param in self._params:\n",
    "                param.state_dict[\"momentum\"] = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        for p in self._params:\n",
    "            if self.momentum:\n",
    "                # SGD with Momentum\n",
    "                # START TODO ################ \n",
    "                p.state_dict[\"momentum\"] *= self.momentum\n",
    "                p.state_dict[\"momentum\"] -= self.lr * p.grad\n",
    "                p.data += p.state_dict[\"momentum\"]\n",
    "                # END TODO ##################\n",
    "            else:\n",
    "                # Standard SGD\n",
    "                # START TODO ################\n",
    "                p.data -= p.grad * self.lr\n",
    "                # END TODO ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"Loads the data, returns training_data, validation_data, test_data.\"\"\"\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "def minibatched(data: np.ndarray, batch_size: int) -> List[np.ndarray]:\n",
    "    assert len(data) % batch_size == 0, (\"Data length {} is not multiple of batch size {}\"\n",
    "                                         .format(len(data), batch_size))\n",
    "    return data.reshape(-1, batch_size, *data.shape[1:])\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_mnist_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = one_hot_encoding(y_train, num_classes)\n",
    "y_val = one_hot_encoding(y_val, num_classes)\n",
    "y_test = one_hot_encoding(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Implement the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, predictions, y_is_onehot: bool = False) -> float:\n",
    "    y_predicted = np.argmax(predictions, axis=-1)\n",
    "    y = np.argmax(y, axis=-1)\n",
    "    return np.sum(np.equal(y_predicted, y)) / len(y)\n",
    "\n",
    "\n",
    "def evaluate(data, labels, model, batch_size):\n",
    "    predictions = []\n",
    "    eval_cost = 0.\n",
    "    data_batched = minibatched(data, batch_size)\n",
    "    labels_batched = minibatched(labels, batch_size)\n",
    "\n",
    "    for x, y in zip(data_batched, labels_batched):\n",
    "        # note that when using cross entropy loss, the softmax is included in the\n",
    "        # loss and we'd need to apply it manually here to obtain the output as probabilities.\n",
    "        # However, softmax only rescales the outputs and doesn't change the argmax,\n",
    "        # so we'll skip this here, as we're only interested in the class prediction.\n",
    "        h_1 = np.squeeze(model(x))\n",
    "        predictions.append(h_1)\n",
    "        eval_cost += loss_fn(h_1, y)\n",
    "    predictions = np.array(predictions).reshape(-1, 10)\n",
    "    eval_accuracy = accuracy(y_val, predictions, False)\n",
    "    return eval_accuracy, eval_cost\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, x_train, y_train, x_val, y_val, num_epochs, batch_size):\n",
    "    train_costs, train_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    eval_costs, eval_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    ix = np.arange(len(x_train))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {} / {}:\".format(epoch + 1, num_epochs))\n",
    "        training_predictions = []\n",
    "        \n",
    "        np.random.shuffle(ix)\n",
    "        x_train_batched = minibatched(x_train[ix], batch_size)\n",
    "        y_train_batched = minibatched(y_train[ix], batch_size)\n",
    "        \n",
    "        # train for one epoch\n",
    "        for x_batch, y_batch in zip(x_train_batched, y_train_batched):\n",
    "            # START TODO ################\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = np.squeeze(model.forward(x_batch))\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            grad_wrt_y_pred = loss_fn.backward()\n",
    "            model.backward(grad_wrt_y_pred)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            training_predictions.append(y_pred)\n",
    "            # TIPP: Look at the optimizer's docstring about how to use it.\n",
    "            #       You might have to add / remove an axis.\n",
    "            # END TODO ##################\n",
    "            train_costs[epoch] += loss\n",
    "       \n",
    "        training_predictions = np.array(training_predictions).reshape(-1, 10)\n",
    "        train_accuracies[epoch] = accuracy(y_train[ix], training_predictions, False)\n",
    "        print(\"  Training Accuracy: {:.4f}\".format(train_accuracies[epoch]))\n",
    "        print(\"  Training Cost: {:.4f}\".format(train_costs[epoch]))\n",
    "        eval_accuracies[epoch], eval_costs[epoch] = evaluate(x_val, y_val, model, batch_size)\n",
    "        print(\"  Eval Accuracy: {:.4f}\".format(eval_accuracies[epoch]))\n",
    "    return train_costs, train_accuracies, eval_costs, eval_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your feedback on exercise 2.2: ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Apply it on a sequential model (linear, sigmoid, linear, softmax (included in cross entropy!). Use the cross entropy loss and sgd with momentum. Use the hyperparameters defined below.\n",
    "\n",
    "What is the best accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init / self.W.shape (30, 784)\n",
      "init / self.b.shape (30, 1)\n",
      "init / self.W.shape (10, 30)\n",
      "init / self.b.shape (10, 1)\n",
      "Epoch 1 / 10:\n",
      "  Training Accuracy: 0.8454\n",
      "  Training Cost: 488.8207\n",
      "  Eval Accuracy: 0.9414\n",
      "Epoch 2 / 10:\n",
      "  Training Accuracy: 0.9418\n",
      "  Training Cost: 194.8214\n",
      "  Eval Accuracy: 0.9521\n",
      "Epoch 3 / 10:\n",
      "  Training Accuracy: 0.9537\n",
      "  Training Cost: 156.2808\n",
      "  Eval Accuracy: 0.9573\n",
      "Epoch 4 / 10:\n",
      "  Training Accuracy: 0.9603\n",
      "  Training Cost: 133.2134\n",
      "  Eval Accuracy: 0.9605\n",
      "Epoch 5 / 10:\n",
      "  Training Accuracy: 0.9644\n",
      "  Training Cost: 117.5845\n",
      "  Eval Accuracy: 0.9618\n",
      "Epoch 6 / 10:\n",
      "  Training Accuracy: 0.9686\n",
      "  Training Cost: 106.2893\n",
      "  Eval Accuracy: 0.9592\n",
      "Epoch 7 / 10:\n",
      "  Training Accuracy: 0.9709\n",
      "  Training Cost: 97.6905\n",
      "  Eval Accuracy: 0.9629\n",
      "Epoch 8 / 10:\n",
      "  Training Accuracy: 0.9733\n",
      "  Training Cost: 90.3283\n",
      "  Eval Accuracy: 0.9635\n",
      "Epoch 9 / 10:\n",
      "  Training Accuracy: 0.9747\n",
      "  Training Cost: 84.4653\n",
      "  Eval Accuracy: 0.9636\n",
      "Epoch 10 / 10:\n",
      "  Training Accuracy: 0.9755\n",
      "  Training Cost: 79.7742\n",
      "  Eval Accuracy: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([488.82067106, 194.82139432, 156.28080494, 133.21340931,\n",
       "        117.58452277, 106.28927265,  97.69053218,  90.32830346,\n",
       "         84.46530285,  79.77424735]),\n",
       " array([0.8454 , 0.94182, 0.9537 , 0.96026, 0.96442, 0.96856, 0.97088,\n",
       "        0.97326, 0.9747 , 0.97548]),\n",
       " array([41.25307483, 33.55995999, 31.42191122, 27.92773748, 26.2668601 ,\n",
       "        28.48282183, 25.06133743, 25.40359597, 25.12185492, 25.00587343]),\n",
       " array([0.9414, 0.9521, 0.9573, 0.9605, 0.9618, 0.9592, 0.9629, 0.9635,\n",
       "        0.9636, 0.9629]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "linear_units = 30\n",
    "\n",
    "# START TODO ################ \n",
    "# Train the model here!\n",
    "model = Sequential(Linear(784,linear_units), Sigmoid(), Linear(linear_units,10))\n",
    "params = model.parameters()\n",
    "optim = SGD(params, learning_rate, momentum)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(model, loss_fn, optim, x_train, y_train, x_val, y_val, num_epochs, batch_size)\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run again with the increased learning rate. Which accuracy can you achieve now? How did the accuracies change while training, compared to the lower learning rate?\n",
    "\n",
    "**Note:** Make sure, to reinitialize your model's parameters. Don't continue on the already trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init / self.W.shape (30, 784)\n",
      "init / self.b.shape (30, 1)\n",
      "init / self.W.shape (10, 30)\n",
      "init / self.b.shape (10, 1)\n",
      "Epoch 1 / 10:\n",
      "  Training Accuracy: 0.1027\n",
      "  Training Cost: 2456.9829\n",
      "  Eval Accuracy: 0.1009\n",
      "Epoch 2 / 10:\n",
      "  Training Accuracy: 0.0996\n",
      "  Training Cost: 2457.1233\n",
      "  Eval Accuracy: 0.0915\n",
      "Epoch 3 / 10:\n",
      "  Training Accuracy: 0.1003\n",
      "  Training Cost: 2457.5641\n",
      "  Eval Accuracy: 0.0961\n",
      "Epoch 4 / 10:\n",
      "  Training Accuracy: 0.0998\n",
      "  Training Cost: 2442.6465\n",
      "  Eval Accuracy: 0.0961\n",
      "Epoch 5 / 10:\n",
      "  Training Accuracy: 0.1013\n",
      "  Training Cost: 2446.4878\n",
      "  Eval Accuracy: 0.0991\n",
      "Epoch 6 / 10:\n",
      "  Training Accuracy: 0.0995\n",
      "  Training Cost: 2474.5184\n",
      "  Eval Accuracy: 0.0990\n",
      "Epoch 7 / 10:\n",
      "  Training Accuracy: 0.1020\n",
      "  Training Cost: 2456.3870\n",
      "  Eval Accuracy: 0.1064\n",
      "Epoch 8 / 10:\n",
      "  Training Accuracy: 0.1019\n",
      "  Training Cost: 2455.8297\n",
      "  Eval Accuracy: 0.1090\n",
      "Epoch 9 / 10:\n",
      "  Training Accuracy: 0.1029\n",
      "  Training Cost: 2441.7708\n",
      "  Eval Accuracy: 0.1030\n",
      "Epoch 10 / 10:\n",
      "  Training Accuracy: 0.1012\n",
      "  Training Cost: 2457.7769\n",
      "  Eval Accuracy: 0.0983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2456.98286502, 2457.12329938, 2457.56412551, 2442.64654733,\n",
       "        2446.48776544, 2474.51835907, 2456.38701751, 2455.82965711,\n",
       "        2441.7707912 , 2457.77690394]),\n",
       " array([0.10272, 0.09958, 0.10028, 0.09978, 0.1013 , 0.09948, 0.102  ,\n",
       "        0.10188, 0.10294, 0.1012 ]),\n",
       " array([484.39770652, 486.34397314, 504.30921071, 489.5705795 ,\n",
       "        486.59025369, 478.28916195, 515.02089636, 470.58239446,\n",
       "        498.80312966, 471.44173892]),\n",
       " array([0.1009, 0.0915, 0.0961, 0.0961, 0.0991, 0.099 , 0.1064, 0.109 ,\n",
       "        0.103 , 0.0983]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higher_learning_rate = 3\n",
    "\n",
    "# START TODO ################ \n",
    "model = Sequential(Linear(784,linear_units), Sigmoid(), Linear(linear_units,10))\n",
    "params = model.parameters()\n",
    "optim = SGD(params, higher_learning_rate, momentum)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(model, loss_fn, optim, x_train, y_train, x_val, y_val, num_epochs, batch_size)\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train only a subset of 1000 training samples, but for 300 epochs. \n",
    "Plot the costs & accuracies for both training and validation.\n",
    "\n",
    "How do the accuracies change compared to the previous runs?\n",
    "How is this plot related to the term 'overfitting'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init / self.W.shape (30, 784)\n",
      "init / self.b.shape (30, 1)\n",
      "init / self.W.shape (10, 30)\n",
      "init / self.b.shape (10, 1)\n",
      "Epoch 1 / 300:\n",
      "  Training Accuracy: 0.1080\n",
      "  Training Cost: 46.5367\n",
      "  Eval Accuracy: 0.0983\n",
      "Epoch 2 / 300:\n",
      "  Training Accuracy: 0.1150\n",
      "  Training Cost: 45.6981\n",
      "  Eval Accuracy: 0.2912\n",
      "Epoch 3 / 300:\n",
      "  Training Accuracy: 0.3500\n",
      "  Training Cost: 40.8594\n",
      "  Eval Accuracy: 0.4261\n",
      "Epoch 4 / 300:\n",
      "  Training Accuracy: 0.5390\n",
      "  Training Cost: 29.5706\n",
      "  Eval Accuracy: 0.6205\n",
      "Epoch 5 / 300:\n",
      "  Training Accuracy: 0.7070\n",
      "  Training Cost: 20.3387\n",
      "  Eval Accuracy: 0.7460\n",
      "Epoch 6 / 300:\n",
      "  Training Accuracy: 0.8130\n",
      "  Training Cost: 14.5395\n",
      "  Eval Accuracy: 0.8032\n",
      "Epoch 7 / 300:\n",
      "  Training Accuracy: 0.8600\n",
      "  Training Cost: 10.8817\n",
      "  Eval Accuracy: 0.8383\n",
      "Epoch 8 / 300:\n",
      "  Training Accuracy: 0.8950\n",
      "  Training Cost: 8.7118\n",
      "  Eval Accuracy: 0.8518\n",
      "Epoch 9 / 300:\n",
      "  Training Accuracy: 0.8970\n",
      "  Training Cost: 7.5306\n",
      "  Eval Accuracy: 0.8547\n",
      "Epoch 10 / 300:\n",
      "  Training Accuracy: 0.9120\n",
      "  Training Cost: 6.5179\n",
      "  Eval Accuracy: 0.8606\n",
      "Epoch 11 / 300:\n",
      "  Training Accuracy: 0.9270\n",
      "  Training Cost: 5.6857\n",
      "  Eval Accuracy: 0.8655\n",
      "Epoch 12 / 300:\n",
      "  Training Accuracy: 0.9400\n",
      "  Training Cost: 4.9858\n",
      "  Eval Accuracy: 0.8689\n",
      "Epoch 13 / 300:\n",
      "  Training Accuracy: 0.9550\n",
      "  Training Cost: 4.4313\n",
      "  Eval Accuracy: 0.8711\n",
      "Epoch 14 / 300:\n",
      "  Training Accuracy: 0.9620\n",
      "  Training Cost: 3.9392\n",
      "  Eval Accuracy: 0.8752\n",
      "Epoch 15 / 300:\n",
      "  Training Accuracy: 0.9630\n",
      "  Training Cost: 3.6089\n",
      "  Eval Accuracy: 0.8731\n",
      "Epoch 16 / 300:\n",
      "  Training Accuracy: 0.9720\n",
      "  Training Cost: 3.2376\n",
      "  Eval Accuracy: 0.8757\n",
      "Epoch 17 / 300:\n",
      "  Training Accuracy: 0.9780\n",
      "  Training Cost: 2.9192\n",
      "  Eval Accuracy: 0.8755\n",
      "Epoch 18 / 300:\n",
      "  Training Accuracy: 0.9770\n",
      "  Training Cost: 2.7324\n",
      "  Eval Accuracy: 0.8801\n",
      "Epoch 19 / 300:\n",
      "  Training Accuracy: 0.9850\n",
      "  Training Cost: 2.3677\n",
      "  Eval Accuracy: 0.8749\n",
      "Epoch 20 / 300:\n",
      "  Training Accuracy: 0.9830\n",
      "  Training Cost: 2.2188\n",
      "  Eval Accuracy: 0.8748\n",
      "Epoch 21 / 300:\n",
      "  Training Accuracy: 0.9880\n",
      "  Training Cost: 2.0484\n",
      "  Eval Accuracy: 0.8771\n",
      "Epoch 22 / 300:\n",
      "  Training Accuracy: 0.9900\n",
      "  Training Cost: 1.8429\n",
      "  Eval Accuracy: 0.8794\n",
      "Epoch 23 / 300:\n",
      "  Training Accuracy: 0.9910\n",
      "  Training Cost: 1.7064\n",
      "  Eval Accuracy: 0.8773\n",
      "Epoch 24 / 300:\n",
      "  Training Accuracy: 0.9930\n",
      "  Training Cost: 1.5734\n",
      "  Eval Accuracy: 0.8775\n",
      "Epoch 25 / 300:\n",
      "  Training Accuracy: 0.9960\n",
      "  Training Cost: 1.4315\n",
      "  Eval Accuracy: 0.8768\n",
      "Epoch 26 / 300:\n",
      "  Training Accuracy: 0.9940\n",
      "  Training Cost: 1.3473\n",
      "  Eval Accuracy: 0.8792\n",
      "Epoch 27 / 300:\n",
      "  Training Accuracy: 0.9960\n",
      "  Training Cost: 1.2332\n",
      "  Eval Accuracy: 0.8787\n",
      "Epoch 28 / 300:\n",
      "  Training Accuracy: 0.9980\n",
      "  Training Cost: 1.1529\n",
      "  Eval Accuracy: 0.8755\n",
      "Epoch 29 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 1.0714\n",
      "  Eval Accuracy: 0.8785\n",
      "Epoch 30 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 1.0105\n",
      "  Eval Accuracy: 0.8795\n",
      "Epoch 31 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 0.9480\n",
      "  Eval Accuracy: 0.8775\n",
      "Epoch 32 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 0.8739\n",
      "  Eval Accuracy: 0.8796\n",
      "Epoch 33 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 0.8154\n",
      "  Eval Accuracy: 0.8778\n",
      "Epoch 34 / 300:\n",
      "  Training Accuracy: 0.9990\n",
      "  Training Cost: 0.7809\n",
      "  Eval Accuracy: 0.8783\n",
      "Epoch 35 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.7329\n",
      "  Eval Accuracy: 0.8789\n",
      "Epoch 36 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.6921\n",
      "  Eval Accuracy: 0.8781\n",
      "Epoch 37 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.6794\n",
      "  Eval Accuracy: 0.8798\n",
      "Epoch 38 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.6238\n",
      "  Eval Accuracy: 0.8798\n",
      "Epoch 39 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.5907\n",
      "  Eval Accuracy: 0.8780\n",
      "Epoch 40 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.5598\n",
      "  Eval Accuracy: 0.8779\n",
      "Epoch 41 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.5364\n",
      "  Eval Accuracy: 0.8786\n",
      "Epoch 42 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.5131\n",
      "  Eval Accuracy: 0.8783\n",
      "Epoch 43 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4955\n",
      "  Eval Accuracy: 0.8778\n",
      "Epoch 44 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4791\n",
      "  Eval Accuracy: 0.8782\n",
      "Epoch 45 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4562\n",
      "  Eval Accuracy: 0.8788\n",
      "Epoch 46 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4350\n",
      "  Eval Accuracy: 0.8786\n",
      "Epoch 47 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4199\n",
      "  Eval Accuracy: 0.8796\n",
      "Epoch 48 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.4044\n",
      "  Eval Accuracy: 0.8788\n",
      "Epoch 49 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3924\n",
      "  Eval Accuracy: 0.8797\n",
      "Epoch 50 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3774\n",
      "  Eval Accuracy: 0.8793\n",
      "Epoch 51 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3643\n",
      "  Eval Accuracy: 0.8795\n",
      "Epoch 52 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3521\n",
      "  Eval Accuracy: 0.8785\n",
      "Epoch 53 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3405\n",
      "  Eval Accuracy: 0.8789\n",
      "Epoch 54 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3313\n",
      "  Eval Accuracy: 0.8798\n",
      "Epoch 55 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3242\n",
      "  Eval Accuracy: 0.8795\n",
      "Epoch 56 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3119\n",
      "  Eval Accuracy: 0.8788\n",
      "Epoch 57 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.3043\n",
      "  Eval Accuracy: 0.8793\n",
      "Epoch 58 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2946\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 59 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2846\n",
      "  Eval Accuracy: 0.8785\n",
      "Epoch 60 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2794\n",
      "  Eval Accuracy: 0.8796\n",
      "Epoch 61 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2729\n",
      "  Eval Accuracy: 0.8796\n",
      "Epoch 62 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2645\n",
      "  Eval Accuracy: 0.8799\n",
      "Epoch 63 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2588\n",
      "  Eval Accuracy: 0.8792\n",
      "Epoch 64 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2523\n",
      "  Eval Accuracy: 0.8796\n",
      "Epoch 65 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2458\n",
      "  Eval Accuracy: 0.8794\n",
      "Epoch 66 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2407\n",
      "  Eval Accuracy: 0.8798\n",
      "Epoch 67 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2350\n",
      "  Eval Accuracy: 0.8795\n",
      "Epoch 68 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2293\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 69 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2248\n",
      "  Eval Accuracy: 0.8802\n",
      "Epoch 70 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2196\n",
      "  Eval Accuracy: 0.8802\n",
      "Epoch 71 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2152\n",
      "  Eval Accuracy: 0.8797\n",
      "Epoch 72 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2093\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 73 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2066\n",
      "  Eval Accuracy: 0.8801\n",
      "Epoch 74 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.2021\n",
      "  Eval Accuracy: 0.8802\n",
      "Epoch 75 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1976\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 76 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1934\n",
      "  Eval Accuracy: 0.8799\n",
      "Epoch 77 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1904\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 78 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1859\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 79 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1828\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 80 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1794\n",
      "  Eval Accuracy: 0.8801\n",
      "Epoch 81 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1768\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 82 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1733\n",
      "  Eval Accuracy: 0.8808\n",
      "Epoch 83 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1700\n",
      "  Eval Accuracy: 0.8805\n",
      "Epoch 84 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1678\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 85 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1640\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 86 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1615\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 87 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1588\n",
      "  Eval Accuracy: 0.8805\n",
      "Epoch 88 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1564\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 89 / 300:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1537\n",
      "  Eval Accuracy: 0.8801\n",
      "Epoch 90 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1515\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 91 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1495\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 92 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1467\n",
      "  Eval Accuracy: 0.8799\n",
      "Epoch 93 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1445\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 94 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1424\n",
      "  Eval Accuracy: 0.8805\n",
      "Epoch 95 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1402\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 96 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1379\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 97 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1361\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 98 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1341\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 99 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1322\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 100 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1304\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 101 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1290\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 102 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1265\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 103 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1252\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 104 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1237\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 105 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1216\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 106 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1206\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 107 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1185\n",
      "  Eval Accuracy: 0.8802\n",
      "Epoch 108 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1174\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 109 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1158\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 110 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1142\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 111 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1131\n",
      "  Eval Accuracy: 0.8800\n",
      "Epoch 112 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1113\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 113 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1101\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 114 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1091\n",
      "  Eval Accuracy: 0.8805\n",
      "Epoch 115 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1076\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 116 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1064\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 117 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1051\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 118 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1042\n",
      "  Eval Accuracy: 0.8805\n",
      "Epoch 119 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1030\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 120 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1017\n",
      "  Eval Accuracy: 0.8802\n",
      "Epoch 121 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.1003\n",
      "  Eval Accuracy: 0.8804\n",
      "Epoch 122 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0994\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 123 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0982\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 124 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0974\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 125 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0963\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 126 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0952\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 127 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0941\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 128 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0931\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 129 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0924\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 130 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0914\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 131 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0903\n",
      "  Eval Accuracy: 0.8803\n",
      "Epoch 132 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0893\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 133 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0886\n",
      "  Eval Accuracy: 0.8808\n",
      "Epoch 134 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0877\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 135 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0869\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 136 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0859\n",
      "  Eval Accuracy: 0.8807\n",
      "Epoch 137 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0852\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 138 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0844\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 139 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0836\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 140 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0828\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 141 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0821\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 142 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0812\n",
      "  Eval Accuracy: 0.8806\n",
      "Epoch 143 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0806\n",
      "  Eval Accuracy: 0.8808\n",
      "Epoch 144 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0798\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 145 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0790\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 146 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0784\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 147 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0776\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 148 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0770\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 149 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0764\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 150 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0757\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 151 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0751\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 152 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0744\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 153 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0738\n",
      "  Eval Accuracy: 0.8818\n",
      "Epoch 154 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0731\n",
      "  Eval Accuracy: 0.8808\n",
      "Epoch 155 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0726\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 156 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0719\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 157 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0713\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 158 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0707\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 159 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0702\n",
      "  Eval Accuracy: 0.8818\n",
      "Epoch 160 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0697\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 161 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0690\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 162 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0685\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 163 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0680\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 164 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0674\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 165 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0669\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 166 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0664\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 167 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0659\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 168 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0654\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 169 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0649\n",
      "  Eval Accuracy: 0.8808\n",
      "Epoch 170 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0644\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 171 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0640\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 172 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0635\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 173 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0630\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 174 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0626\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 175 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0621\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 176 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0617\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 177 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0612\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 178 / 300:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0607\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 179 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0603\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 180 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0598\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 181 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0594\n",
      "  Eval Accuracy: 0.8809\n",
      "Epoch 182 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0591\n",
      "  Eval Accuracy: 0.8817\n",
      "Epoch 183 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0587\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 184 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0582\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 185 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0579\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 186 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0574\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 187 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0571\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 188 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0567\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 189 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0563\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 190 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0559\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 191 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0556\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 192 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0552\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 193 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0549\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 194 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0545\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 195 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0542\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 196 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0538\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 197 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0536\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 198 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0531\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 199 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0528\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 200 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0525\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 201 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0522\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 202 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0518\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 203 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0516\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 204 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0513\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 205 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0509\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 206 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0506\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 207 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0503\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 208 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0500\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 209 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0497\n",
      "  Eval Accuracy: 0.8818\n",
      "Epoch 210 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0494\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 211 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0491\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 212 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0489\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 213 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0485\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 214 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0482\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 215 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0480\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 216 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0477\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 217 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0475\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 218 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0472\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 219 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0469\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 220 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0466\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 221 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0464\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 222 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0461\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 223 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0459\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 224 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0456\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 225 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0453\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 226 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0452\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 227 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0449\n",
      "  Eval Accuracy: 0.8817\n",
      "Epoch 228 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0447\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 229 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0444\n",
      "  Eval Accuracy: 0.8810\n",
      "Epoch 230 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0442\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 231 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0439\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 232 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0437\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 233 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0435\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 234 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0433\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 235 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0430\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 236 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0428\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 237 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0425\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 238 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0423\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 239 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0421\n",
      "  Eval Accuracy: 0.8817\n",
      "Epoch 240 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0419\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 241 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0417\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 242 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0415\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 243 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0413\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 244 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0411\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 245 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0409\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 246 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0407\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 247 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0405\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 248 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0403\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 249 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0401\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 250 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0399\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 251 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0397\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 252 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0395\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 253 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0393\n",
      "  Eval Accuracy: 0.8817\n",
      "Epoch 254 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0391\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 255 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0389\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 256 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0388\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 257 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0386\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 258 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0384\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 259 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0382\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 260 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0380\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 261 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0379\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 262 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0377\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 263 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0375\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 264 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0373\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 265 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0371\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 266 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0370\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 267 / 300:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0368\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 268 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0366\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 269 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0365\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 270 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0363\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 271 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0361\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 272 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0360\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 273 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0359\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 274 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0357\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 275 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0356\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 276 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0354\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 277 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0353\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 278 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0350\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 279 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0349\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 280 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0348\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 281 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0346\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 282 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0345\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 283 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0343\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 284 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0342\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 285 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0340\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 286 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0339\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 287 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0337\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 288 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0336\n",
      "  Eval Accuracy: 0.8816\n",
      "Epoch 289 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0335\n",
      "  Eval Accuracy: 0.8811\n",
      "Epoch 290 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0333\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 291 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0332\n",
      "  Eval Accuracy: 0.8812\n",
      "Epoch 292 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0331\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 293 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0329\n",
      "  Eval Accuracy: 0.8815\n",
      "Epoch 294 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0328\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 295 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0326\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 296 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0325\n",
      "  Eval Accuracy: 0.8814\n",
      "Epoch 297 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0324\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 298 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0322\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 299 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0321\n",
      "  Eval Accuracy: 0.8813\n",
      "Epoch 300 / 300:\n",
      "  Training Accuracy: 1.0000\n",
      "  Training Cost: 0.0320\n",
      "  Eval Accuracy: 0.8814\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = 1000\n",
    "num_epochs = 300\n",
    "\n",
    "# START TODO ################ \n",
    "model = Sequential(Linear(784,linear_units), Sigmoid(), Linear(linear_units,10))\n",
    "params = model.parameters()\n",
    "optim = SGD(params, learning_rate, momentum)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "x_mini = x_train[:num_train_samples]\n",
    "y_mini = y_train[:num_train_samples]\n",
    "\n",
    "tc, ta, vc, va = train(model, loss_fn, optim, x_mini, y_mini, x_val, y_val, num_epochs, batch_size)\n",
    "# END TODO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(tc, label='training')\n",
    "plt.plot(vc, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(2)\n",
    "# Plot accuracies here\n",
    "# START TODO ################ \n",
    "plt.plot(ta, label='training')\n",
    "plt.plot(va, label='validation')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "# END TODO ##################\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your feedback on exercise 2.3: ** \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
